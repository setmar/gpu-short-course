{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c689b1c-ef04-4373-b66a-ff8a42b8b980",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute pi on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201a23d-3c2b-4bf6-9e0e-185bf7b95c20",
   "metadata": {},
   "source": [
    "0. CPU version: about 18 seconds for 512 000 000 points\n",
    "1. Maximum 1024 threads (one block with 1024 threads)\n",
    "2. Slow with 512 000 000 threads, 1.5 seconds (n blocks with 512 threads each)\n",
    "3. Less memory. 512 000 000 in 0.029076 seconds (shared memory reduction). About factor 50 speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iQzuLsxx9FxI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQzuLsxx9FxI",
    "outputId": "7c39b11c-d4a2-4435-f43c-58cce52cad3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m1.4/1.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
      "Downloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp311-cp311-linux_x86_64.whl size=660362 sha256=05937a5ad9041c40de0244fb6c18563ee316772338aeddc861d0a315841950d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/66/50/c65e6116d7e0e16abe0f7c19b50327f76724ccfefbdc61a1b9\n",
      "Successfully built pycuda\n",
      "Installing collected packages: pytools, mako, pycuda\n",
      "Successfully installed mako-1.3.8 pycuda-2024.1.2 pytools-2025.1.1\n",
      "CPU times: user 754 ms, sys: 131 ms, total: 884 ms\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b295c1-cd41-4103-8827-a96fd2316b91",
   "metadata": {
    "id": "b3b295c1-cd41-4103-8827-a96fd2316b91"
   },
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c33611-eeb8-4820-bce2-b43d6a34d4ba",
   "metadata": {
    "id": "80c33611-eeb8-4820-bce2-b43d6a34d4ba"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "def compute_pi_cpu(n_points):\n",
    "    #First, generate random points\n",
    "    rng = np.random.RandomState(42)\n",
    "    x_rand = rng.random(n_points)\n",
    "    y_rand = rng.random(n_points)\n",
    "\n",
    "    #Compute radius from origin\n",
    "    inside = np.sqrt(x_rand**2+y_rand**2) <= 1.0\n",
    "    #Count number of points inside\n",
    "    n_inside = np.sum(inside)\n",
    "\n",
    "    #n_inside = 0\n",
    "    #for i in range(n_points):\n",
    "    #    n_inside += np.sqrt(x_rand[i]**2+y_rand[i]**2) <= 1.0\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    #pi = 4 * n_inside / n_total\n",
    "    pi = 4*n_inside/n_points\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24ca2c7-3aed-4031-a7ec-969e06f7bc57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c24ca2c7-3aed-4031-a7ec-969e06f7bc57",
    "outputId": "33cfab91-e55f-439c-b217-0d0ed48397fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.142240625\n",
      "Time to execute cpu version: 7.615318 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_cpu(5120000))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time to execute cpu version: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa19d57b-0bd0-4f18-998e-ec4a0a974b20",
   "metadata": {
    "id": "aa19d57b-0bd0-4f18-998e-ec4a0a974b20"
   },
   "outputs": [],
   "source": [
    "pi_kernel_src = \"\"\"\n",
    "//Based on Stroustrup, adapted for CUDA\n",
    "//pseudorandom numbers\n",
    "__device__ float generateRandomNumber(long& last_draw) {\n",
    "    last_draw = last_draw*1103515245 + 12345;\n",
    "    long abs = last_draw & 0x7fffffff;\n",
    "    return abs / 2147483648.0;\n",
    "}\n",
    "\n",
    "__global__ void computePi(unsigned int* inside, unsigned int seed) {\n",
    "    __shared__ unsigned int inside_shared[512];\n",
    "\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int bid = blockIdx.x;\n",
    "\n",
    "    //1 generate random numbers\n",
    "    unsigned int num_inside = 0;\n",
    "    long rand_seed = seed + blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    float x = generateRandomNumber(rand_seed);\n",
    "    float y = generateRandomNumber(rand_seed);\n",
    "\n",
    "    //2 compute the radius from the origin\n",
    "    float r = sqrt(x*x + y*y);\n",
    "\n",
    "    //3 check if inside circle and write to memory\n",
    "    if (r <= 1) {\n",
    "            num_inside += 1;\n",
    "    }\n",
    "    inside_shared[tid] = num_inside;\n",
    "\n",
    "    /////////////////////////\n",
    "    //Shared memory reduction\n",
    "    /////////////////////////\n",
    "\n",
    "    // Synchronze so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "\n",
    "    // Find the sum in shared memory\n",
    "    //Reduce from 512 to 256 elements\n",
    "    if (threadIdx.x < 256) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 256];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 256 to 128 elements\n",
    "    if (threadIdx.x < 128) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 128];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 64];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 32 to 16 elements\n",
    "    //Since we here have only one active warp (threadIdx.x > 32)\n",
    "    //we do not need to call syncthreads anymore\n",
    "    volatile unsigned int* p = &inside_shared[0]; //To help the compiler not cache this variable...\n",
    "    if (threadIdx.x < 32) {\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 32];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 16];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 8];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 4];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 2];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 1];\n",
    "    }\n",
    "\n",
    "    // Finally write out to output\n",
    "    // NOTE: We have 512 threads, but only thread 0 writes to memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        inside[bid] = p[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "mod = SourceModule(pi_kernel_src)\n",
    "func = mod.get_function(\"computePi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6291ac3a-64c9-414f-bec0-37284fb660bd",
   "metadata": {
    "id": "6291ac3a-64c9-414f-bec0-37284fb660bd"
   },
   "outputs": [],
   "source": [
    "def compute_pi_gpu(n_points, threads_per_block=512):\n",
    "    assert(n_points % threads_per_block == 0)\n",
    "\n",
    "    #Allocate output data on the GPU\n",
    "    #Bytes per unsigned int:\n",
    "    bytes_per_uint = 4\n",
    "    inside_gpu = cuda.mem_alloc(bytes_per_uint*(n_points//threads_per_block))\n",
    "\n",
    "    #Execute the pi-kernel\n",
    "    num_blocks = n_points//threads_per_block\n",
    "    block=(threads_per_block,1,1)\n",
    "    grid=(num_blocks,1,1)\n",
    "    func(inside_gpu, np.uint32(time.time()), block=(threads_per_block,1,1), grid=(num_blocks,1,1))\n",
    "\n",
    "    #Allocate memory to download to on the CPU\n",
    "    inside_cpu = np.empty((n_points//threads_per_block), dtype=np.uint32)\n",
    "\n",
    "    #Download from the GPU to the CPU\n",
    "    cuda.memcpy_dtoh(inside_cpu, inside_gpu)\n",
    "\n",
    "    #Count number of points inside\n",
    "    n_inside = np.sum(inside_cpu)\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    pi = 4*n_inside/n_points\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f895258-7fd0-4814-888d-97b36eec8426",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f895258-7fd0-4814-888d-97b36eec8426",
    "outputId": "eabe9e5e-ed1e-4342-be77-a9367b1a1973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1416015625\n",
      "Time to execute gpu version: 0.001817 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_gpu(5120000, 512))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time to execute gpu version: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a6c31-045c-4791-94a7-e642b2102a1c",
   "metadata": {
    "id": "989a6c31-045c-4791-94a7-e642b2102a1c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
