{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b295c1-cd41-4103-8827-a96fd2316b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c33611-eeb8-4820-bce2-b43d6a34d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi_cpu(n_points):\n",
    "    x_rand = np.random.rand(n_points)\n",
    "    y_rand = np.random.rand(n_points)\n",
    "\n",
    "    n_inside = 0\n",
    "    for i in range(n_points):\n",
    "        n_inside += np.sqrt(x_rand[i]**2 + y_rand[i]**2) <= 1.0\n",
    "\n",
    "    pi = 4 * n_inside/n_points\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c24ca2c7-3aed-4031-a7ec-969e06f7bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12890625\n",
      "Time to execute cpu version: 0.010859 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_cpu(1024))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time to execute cpu version: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa19d57b-0bd0-4f18-998e-ec4a0a974b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_kernel_src = \"\"\"\n",
    "//Based on Stroustrup, adapted for CUDA\n",
    "//pseudorandom numbers\n",
    "__device__ float generateRandomNumber(long& last_draw) {\n",
    "    last_draw = last_draw*1103515245 + 12345;\n",
    "    long abs = last_draw & 0x7fffffff;\n",
    "    return abs / 2147483648.0; \n",
    "}\n",
    "\n",
    "__global__ void computePi(unsigned int* inside, unsigned int seed) {\n",
    "    __shared__ unsigned int inside_shared[512];\n",
    "    \n",
    "    //1 generate random numbers\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    \n",
    "    long rand_seed = seed + tid;\n",
    "    float x = generateRandomNumber(rand_seed);\n",
    "    float y = generateRandomNumber(rand_seed);\n",
    "    \n",
    "    //2 compute the radius from the origin\n",
    "    float r = sqrt(x*x + y*y);\n",
    "    \n",
    "    //3 check if inside circle and write to memory\n",
    "    if (r <= 1) {\n",
    "            num_inside += 1;\n",
    "    }\n",
    "    inside_shared[tid] = num_inside;\n",
    "\n",
    "    /////////////////////////\n",
    "    //Shared memory reduction\n",
    "    /////////////////////////\n",
    "\n",
    "    // Synchronze so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "\n",
    "    // Find the sum in shared memory\n",
    "    //Reduce from 512 to 256 elements\n",
    "    if (threadIdx.x < 256) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 256];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 256 to 128 elements\n",
    "    if (threadIdx.x < 128) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 128];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 64];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 32 to 16 elements\n",
    "    //Since we here have only one active warp (threadIdx.x > 32)\n",
    "    //we do not need to call syncthreads anymore\n",
    "    volatile unsigned int* p = &inside_shared[0]; //To help the compiler not cache this variable...\n",
    "    if (threadIdx.x < 32) {\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 32];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 16];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 8];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 4];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 2];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 1];\n",
    "    }\n",
    "\n",
    "    // Finally write out to output\n",
    "    // NOTE: We have 512 threads, but only thread 0 writes to memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        inside[bid] = p[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "mod = SourceModule(pi_kernel_src)\n",
    "func = mod.get_function(\"computePi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6291ac3a-64c9-414f-bec0-37284fb660bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi_gpu(n_points, threads_per_block=512):\n",
    "    assert(n_points % threads_per_block == 0)\n",
    "\n",
    "    #Allocate output data on the GPU\n",
    "    #Bytes per unsigned int:\n",
    "    bytes_per_uint = 4\n",
    "    inside_gpu = cuda.mem_alloc(bytes_per_uint*(n_points//threads_per_block))\n",
    "\n",
    "    #Execute the pi-kernel\n",
    "    num_blocks = n_points//threads_per_block\n",
    "    block=(threads_per_block,1,1)\n",
    "    grid=(num_blocks,1,1)\n",
    "    compute_pi_gpu_kernel(inside_gpu, np.uint32(iterations_per_thread), np.uint32(time.time()), block=(threads_per_block,1,1), grid=(num_blocks,1,1))\n",
    "    \n",
    "    #Allocate memory to download to on the CPU\n",
    "    inside_cpu = np.empty((n_points//threads_per_block), dtype=np.uint32)\n",
    "\n",
    "    #Download from the GPU to the CPU\n",
    "    cuda.memcpy_dtoh(inside_cpu, inside_gpu)\n",
    "\n",
    "    #Count number of points inside\n",
    "    n_inside = np.sum(inside_cpu)\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    pi = 4*n_inside/n_points\n",
    "\n",
    "    return pi\n",
    "\n",
    "tic = time.time()\n",
    "print(compute_pi_gpu(5120000, 10000, 512))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time taken: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f895258-7fd0-4814-888d-97b36eec8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "Time to execute gpu version: 0.001571 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_gpu(1024))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time to execute gpu version: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a6c31-045c-4791-94a7-e642b2102a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
