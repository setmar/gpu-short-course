{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b073e954-50dc-4a26-8fba-0cb4235aeba1",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute pi on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a11b2-6529-4693-a559-57b585c1fc2f",
   "metadata": {},
   "source": [
    "0. CPU version: about 18 seconds for 512 000 000 points\n",
    "1. Maximum 1024 threads (one block with 1024 threads)\n",
    "2. Slow with 512 000 000 threads, 1.5 seconds (n blocks with 512 threads each)\n",
    "3. Less memory. 512 000 000 in 0.029076 seconds (shared memory reduction). About factor 50 speedup\n",
    "4. More work per thread (less memory). 512 000 000 in 0.003880 seconds (for-loop in kernel). About factor 10 speedup (or more!) uses 2.16 seconds for 51 200 000 000 000 darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "slT7YI74BCc-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slT7YI74BCc-",
    "outputId": "4b5d94ba-e985-4fed-e518-a68618a97861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
      "Downloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp311-cp311-linux_x86_64.whl size=660362 sha256=5de71b1d9cb10d70787deb7c81e0ba9a81d24ff7c65243b1f5151baef049ccb5\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/66/50/c65e6116d7e0e16abe0f7c19b50327f76724ccfefbdc61a1b9\n",
      "Successfully built pycuda\n",
      "Installing collected packages: pytools, mako, pycuda\n",
      "Successfully installed mako-1.3.8 pycuda-2024.1.2 pytools-2025.1.1\n",
      "CPU times: user 733 ms, sys: 95.7 ms, total: 829 ms\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bbb65e-eead-424c-ac73-e7666d5ae0be",
   "metadata": {
    "id": "06bbb65e-eead-424c-ac73-e7666d5ae0be"
   },
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993c47d5-91e3-4129-893a-549064bc57c9",
   "metadata": {
    "id": "993c47d5-91e3-4129-893a-549064bc57c9"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "def compute_pi_cpu(n_points):\n",
    "    #First, generate random points\n",
    "    rng = np.random.RandomState(42)\n",
    "    x_rand = rng.random(n_points)\n",
    "    y_rand = rng.random(n_points)\n",
    "\n",
    "    #Compute radius from origin\n",
    "    inside = np.sqrt(x_rand**2+y_rand**2) <= 1.0\n",
    "    #Count number of points inside\n",
    "    n_inside = np.sum(inside)\n",
    "\n",
    "    #n_inside = 0\n",
    "    #for i in range(n_points):\n",
    "    #    n_inside += np.sqrt(x_rand[i]**2+y_rand[i]**2) <= 1.0\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    #pi = 4 * n_inside / n_total\n",
    "    pi = 4*n_inside/n_points\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbe45c6-eb3a-4179-a1b0-8c5a6328b35e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bbe45c6-eb3a-4179-a1b0-8c5a6328b35e",
    "outputId": "856f0cf5-579b-42d4-ccfe-4afeda31e4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1418\n",
      "Time taken: 0.148854 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_cpu(5120000))\n",
    "toc = time.time()\n",
    "\n",
    "#for loop: 1.84 seconds for 512 000 elements\n",
    "#vectorized: 0.018 seconds for 512 000 elements => 100 times faster!!!\n",
    "\n",
    "print(\"Time taken: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75fce2ec-ed1a-4650-8ef4-473c38d9933f",
   "metadata": {
    "id": "75fce2ec-ed1a-4650-8ef4-473c38d9933f"
   },
   "outputs": [],
   "source": [
    "pi_kernel_src = \"\"\"\n",
    "//Based on Stroustrup, adapted for CUDA\n",
    "//pseudorandom numbers\n",
    "__device__ float generateRandomNumber(long& last_draw) {\n",
    "    last_draw = last_draw*1103515245 + 12345;\n",
    "    long abs = last_draw & 0x7fffffff;\n",
    "    return abs / 2147483648.0;\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void computePi(unsigned int* inside, unsigned int num_iterations, unsigned int seed) {\n",
    "    __shared__ unsigned int inside_shared[512];\n",
    "\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int bid = blockIdx.x;\n",
    "\n",
    "    //1 generate random numbers\n",
    "    unsigned int num_inside = 0;\n",
    "    for (int i=0; i<num_iterations; ++i) {\n",
    "        long rand_seed = seed + blockIdx.x*blockDim.x + threadIdx.x;\n",
    "        float x = generateRandomNumber(rand_seed);\n",
    "        float y = generateRandomNumber(rand_seed);\n",
    "\n",
    "        //2 compute radius from origin\n",
    "        float r = sqrt(x*x+y*y);\n",
    "\n",
    "        //3 check if inside circle and write to memory\n",
    "        if (r <= 1) {\n",
    "            num_inside += 1;\n",
    "        }\n",
    "    }\n",
    "    inside_shared[tid] = num_inside;\n",
    "\n",
    "    /////////////////////////\n",
    "    //Shared memory reduction\n",
    "    /////////////////////////\n",
    "\n",
    "    // Synchronze so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "\n",
    "    // Find the sum in shared memory\n",
    "    //Reduce from 512 to 256 elements\n",
    "    if (threadIdx.x < 256) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 256];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 256 to 128 elements\n",
    "    if (threadIdx.x < 128) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 128];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 64];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    //Reduce from 32 to 16 elements\n",
    "    //Since we here have only one active warp (threadIdx.x > 32)\n",
    "    //we do not need to call syncthreads anymore\n",
    "    volatile unsigned int* p = &inside_shared[0]; //To help the compiler not cache this variable...\n",
    "    if (threadIdx.x < 32) {\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 32];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 16];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 8];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 4];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 2];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 1];\n",
    "    }\n",
    "\n",
    "    // Finally write out to output\n",
    "    // NOTE: We have 512 threads, but only thread 0 writes to memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        inside[bid] = p[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "mod = SourceModule(pi_kernel_src)\n",
    "compute_pi_gpu_kernel = mod.get_function(\"computePi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936395a1-3289-4b9b-8610-6db971f196d6",
   "metadata": {
    "id": "936395a1-3289-4b9b-8610-6db971f196d6"
   },
   "outputs": [],
   "source": [
    "def compute_pi_gpu(n_points, iterations_per_thread=1000, threads_per_block=512):\n",
    "    #10 000 000 points gave 3.141932\n",
    "    #100 000 000 points gave 3.14141676\n",
    "    #n_total = 1000000\n",
    "\n",
    "    assert(n_points % (threads_per_block*iterations_per_thread) == 0)\n",
    "\n",
    "    #Allocate output data on the GPU\n",
    "    #Bytes per unsigned int:\n",
    "    bytes_per_uint = 4\n",
    "    inside_gpu = cuda.mem_alloc(bytes_per_uint*(n_points//threads_per_block)//iterations_per_thread)\n",
    "\n",
    "    #Execute the pi-kernel\n",
    "    num_blocks = (n_points // threads_per_block)//iterations_per_thread\n",
    "    block=(threads_per_block,1,1)\n",
    "    grid=(num_blocks,1,1)\n",
    "    compute_pi_gpu_kernel(inside_gpu, np.uint32(iterations_per_thread), np.uint32(time.time()), block=(threads_per_block,1,1), grid=(num_blocks,1,1))\n",
    "\n",
    "    #Allocate memory to download to on the CPU\n",
    "    inside_cpu = np.empty((n_points//threads_per_block)//iterations_per_thread, dtype=np.uint32)\n",
    "\n",
    "    #Download from the GPU to the CPU\n",
    "    cuda.memcpy_dtoh(inside_cpu, inside_gpu)\n",
    "\n",
    "    #Count number of points inside\n",
    "    # Version 6: move this reduction to the GPU, and only transfer a single number to the CPU.\n",
    "    n_inside = np.sum(inside_cpu)\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    pi = 4*n_inside/n_points\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed16a63-e7fd-48fe-a1fe-3d001d835c7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ed16a63-e7fd-48fe-a1fe-3d001d835c7c",
    "outputId": "c705f550-0f12-453b-8f32-0ccd40d206dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1328125\n",
      "Time to execute gpu version: 0.001411 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_gpu(5120000, 10000, 512))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time to execute gpu version: {:f} seconds\".format(toc-tic))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
