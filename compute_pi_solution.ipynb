{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5892eb97-ab6e-47de-8786-f331b2039e02",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute pi on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedf9bd-02ec-4b84-8501-abfd6bccd3da",
   "metadata": {},
   "source": [
    "0. CPU version: about 18 seconds for 512 000 000 points\n",
    "1. Maximum 1024 threads (one block with 1024 threads)\n",
    "2. Slow with 512 000 000 threads, 1.5 seconds (n blocks with 512 threads each)\n",
    "3. Less memory. 512 000 000 in 0.029076 seconds (shared memory reduction). About factor 50 speedup\n",
    "4. More work per thread (less memory). 512 000 000 in 0.003880 seconds (for-loop in kernel). About factor 10 speedup (or more!) uses 2.16 seconds for 51 200 000 000 000 darts\n",
    "5. Prepared call kernel launch => no benefit right now. We run the kernel only once!\n",
    "6. Sum/reduction on the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e62cf-497e-4fc8-bffb-9b2272b75b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb65e-eead-424c-ac73-e7666d5ae0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cuda\n",
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c47d5-91e3-4129-893a-549064bc57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi_cpu(n_points):\n",
    "    #10 000 000 points gave 3.141932\n",
    "    #100 000 000 points gave 3.14141676\n",
    "    #n_total = 1000000\n",
    "\n",
    "    #First, generate random points\n",
    "    x_rand = np.random.rand(n_points)\n",
    "    y_rand = np.random.rand(n_points)\n",
    "\n",
    "    #Compute radius from origin\n",
    "    inside = np.sqrt(x_rand**2+y_rand**2) <= 1.0\n",
    "    #Count number of points inside\n",
    "    n_inside = np.sum(inside)\n",
    "    \n",
    "    #n_inside = 0\n",
    "    #for i in range(n_points):\n",
    "    #    n_inside += np.sqrt(x_rand[i]**2+y_rand[i]**2) <= 1.0\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    #pi = 4 * n_inside / n_total\n",
    "    pi = 4*n_inside/n_points\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe45c6-eb3a-4179-a1b0-8c5a6328b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "print(compute_pi_cpu(512000))\n",
    "toc = time.time() \n",
    "\n",
    "#for loop: 1.84 seconds for 512 000 elements\n",
    "#vectorized: 0.018 seconds for 512 000 elements => 100 times faster!!!\n",
    "\n",
    "print(\"Time taken: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fce2ec-ed1a-4650-8ef4-473c38d9933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_kernel_src = \"\"\"\n",
    "//Based on Stroustrup, adapted for CUDA\n",
    "//pseudorandom numbers\n",
    "__device__ float generateRandomNumber(long& last_draw) {\n",
    "    last_draw = last_draw*1103515245 + 12345;\n",
    "    long abs = last_draw & 0x7fffffff;\n",
    "    return abs / 2147483648.0; \n",
    "}\n",
    "\n",
    "\n",
    "__global__ void computePi(unsigned int* inside, unsigned int num_iterations, unsigned int seed) {\n",
    "    __shared__ unsigned int inside_shared[512];\n",
    "    \n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int bid = blockIdx.x;\n",
    "\n",
    "    //1 generate random numbers\n",
    "    unsigned int num_inside = 0;\n",
    "    for (int i=0; i<num_iterations; ++i) {\n",
    "        long rand_seed = seed + blockIdx.x*blockDim.x + threadIdx.x;\n",
    "        float x = generateRandomNumber(rand_seed);\n",
    "        float y = generateRandomNumber(rand_seed);\n",
    "\n",
    "        //2 compute radius from origin\n",
    "        float r = sqrt(x*x+y*y);\n",
    "\n",
    "        //3 check if inside circle and write to memory\n",
    "        if (r <= 1) {\n",
    "            num_inside += 1;\n",
    "        }\n",
    "    }\n",
    "    inside_shared[tid] = num_inside;\n",
    "    \n",
    "    /////////////////////////\n",
    "    //Shared memory reduction\n",
    "    /////////////////////////\n",
    "    \n",
    "    // Synchronze so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the sum in shared memory\n",
    "    //Reduce from 512 to 256 elements\n",
    "    if (threadIdx.x < 256) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 256];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 256 to 128 elements\n",
    "    if (threadIdx.x < 128) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 128];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        inside_shared[threadIdx.x] = inside_shared[threadIdx.x] + inside_shared[threadIdx.x + 64];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 32 to 16 elements\n",
    "    //Since we here have only one active warp (threadIdx.x > 32)\n",
    "    //we do not need to call syncthreads anymore\n",
    "    volatile unsigned int* p = &inside_shared[0]; //To help the compiler not cache this variable...\n",
    "    if (threadIdx.x < 32) {\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 32];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 16];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 8];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 4];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 2];\n",
    "        p[threadIdx.x] = p[threadIdx.x] + p[threadIdx.x + 1];\n",
    "    }\n",
    "    \n",
    "    // Finally write out to output\n",
    "    // NOTE: We have 512 threads, but only thread 0 writes to memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        inside[bid] = p[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "mod = SourceModule(pi_kernel_src)\n",
    "compute_pi_gpu_kernel = mod.get_function(\"computePi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11992b59-f7de-4f0a-98f7-272fe9cf6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a function call: give pycuda instructions\n",
    "#on what type of parameters to send to CUDA\n",
    "#__global__ void computePi(\n",
    "# unsigned int* inside, <= P\n",
    "# unsigned int num_iterations, <= i\n",
    "# unsigned int seed <= i\n",
    "# );\n",
    "compute_pi_gpu_kernel.prepare(\"Pii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936395a1-3289-4b9b-8610-6db971f196d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi_gpu(n_points, iterations_per_thread=1000, threads_per_block=512):\n",
    "    #10 000 000 points gave 3.141932\n",
    "    #100 000 000 points gave 3.14141676\n",
    "    #n_total = 1000000\n",
    "    \n",
    "    assert(n_points % (threads_per_block*iterations_per_thread) == 0)\n",
    "\n",
    "    #Allocate output data on the GPU\n",
    "    #Bytes per unsigned int: \n",
    "    bytes_per_uint = 4\n",
    "    inside_gpu = cuda.mem_alloc(bytes_per_uint*(n_points//threads_per_block)//iterations_per_thread)\n",
    "    \n",
    "    #Execute the pi-kernel\n",
    "    num_blocks = (n_points // threads_per_block)//iterations_per_thread\n",
    "    block=(threads_per_block,1,1)\n",
    "    grid=(num_blocks,1,1)\n",
    "    #compute_pi_gpu_kernel(inside_gpu, np.uint32(iterations_per_thread), np.uint32(time.time()), block=(threads_per_block,1,1), grid=(num_blocks,1,1))\n",
    "    compute_pi_gpu_kernel.prepared_call(grid, block, inside_gpu, np.uint32(iterations_per_thread), np.uint32(time.time()))\n",
    "    \n",
    "    #Allocate memory to download to on the CPU\n",
    "    inside_cpu = np.empty((n_points//threads_per_block)//iterations_per_thread, dtype=np.uint32)\n",
    "    \n",
    "    #Download from the GPU to the CPU\n",
    "    cuda.memcpy_dtoh(inside_cpu, inside_gpu)\n",
    "    \n",
    "    #Count number of points inside\n",
    "    # Version 6: move this reduction to the GPU, and only transfer a single number to the CPU.\n",
    "    n_inside = np.sum(inside_cpu)\n",
    "\n",
    "    #We can estimate pi by the following formula:\n",
    "    pi = 4*n_inside/n_points\n",
    "    \n",
    "    return pi\n",
    "\n",
    "tic = time.time()\n",
    "print(compute_pi_gpu(51200000000000, 10000000, 512))\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Time taken: {:f} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed16a63-e7fd-48fe-a1fe-3d001d835c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
